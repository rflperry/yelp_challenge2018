{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib2\n",
    "url = \"https://raw.githubusercontent.com/suneman/socialgraphs2016/master/files/test.json\"\n",
    "response = urllib2.urlopen(url)\n",
    "listObj = json.load(response)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildWikiQuery(page, last_revision_year):\n",
    "    # Build query\n",
    "    baseurl = \"http://en.wikipedia.org/w/api.php/?\"\n",
    "    action = \"action=query\"\n",
    "    title = \"titles=%s\" % page \n",
    "    content = \"prop=revisions\"\n",
    "    rvprop =\"rvprop=timestamp|content\"\n",
    "    dataformat = \"format=json\"\n",
    "    rvdir = \"rvdir=older\" #sort revisions: old -> new\n",
    "    start = \"rvend=2000-01-03T00:00:00Z\" #start of my time period\n",
    "    end = \"rvstart=%s-01-03T00:00:00Z\" % last_revision_year #end of my time period\n",
    "    limit = \"rvlimit=1\" #consider only the first revision\n",
    "\n",
    "    query = \"%s%s&%s&%s&%s&%s&%s&%s&%s&%s\" % (baseurl, action, title, content, rvprop, dataformat, rvdir, end, start, limit)\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://en.wikipedia.org/w/api.php/?action=query&titles=PAGE&prop=revisions&rvprop=timestamp|content&format=json&rvdir=older&rvstart=2019-01-03T00:00:00Z&rvend=2000-01-03T00:00:00Z&rvlimit=1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buildWikiQuery(\"PAGE\", \"2019\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchContent(query):\n",
    "    # Send query to wikipedia\n",
    "    result = 'No content'\n",
    "    revision_key = 'revisions'\n",
    "    response = urllib2.urlopen(query)\n",
    "    json_reponse = json.load(response)\n",
    "    pages = json_reponse[\"query\"]['pages']\n",
    "    page_key = pages.keys()[0]\n",
    "    page_content = pages[page_key]\n",
    "    if revision_key in page_content:\n",
    "        content = pages[page_key][revision_key]\n",
    "        result = content[0][\"*\"].encode(\"utf8\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveToFile(name, year, content):\n",
    "    if content == 'No content':\n",
    "        print 'Empty page for:', name\n",
    "    else:\n",
    "        filename = 'congress%s/%s.txt' % (year, name)\n",
    "        f = open(filename, \"a\")\n",
    "        f.write(content)\n",
    "        f.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "h113 = pd.read_csv(\"dataset/H113.csv\")\n",
    "h114 = pd.read_csv(\"dataset/H114.csv\")\n",
    "h115 = pd.read_csv(\"dataset/H115.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(dataframe, year):\n",
    "    wikipages = list(set(h113.WikiPageName))\n",
    "    for i, page_name in enumerate(wikipages):\n",
    "        wiki_query = buildWikiQuery(page_name, year)\n",
    "        wiki_content = fetchContent(wiki_query)\n",
    "        saveToFile(page_name, year, wiki_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch(h113, '2015')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch(h114, '2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch(h115, '2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir_name = \"./congress/\"\n",
    "directory = os.listdir(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in directory:\n",
    "    if item.endswith(\"2019.txt\"):\n",
    "        os.remove(os.path.join(dir_name, item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = 1 - w*(1 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
